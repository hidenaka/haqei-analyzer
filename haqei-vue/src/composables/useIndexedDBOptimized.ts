/**
 * useIndexedDBOptimized.ts - é«˜é€ŸIndexedDBæœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ã‚¶ãƒ–ãƒ«
 * 
 * ç›®çš„ï¼š
 * - IndexedDBã‚¢ã‚¯ã‚»ã‚¹é€Ÿåº¦ã‚’70%ä»¥ä¸Šå‘ä¸Š
 * - ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿æ“ä½œ
 * - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜é€Ÿæ¤œç´¢
 * - åœ§ç¸®ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡åŒ–
 * - bunenjinå“²å­¦æº–æ‹ ã®èª¿å’Œã®å–ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–
 * 
 * æ©Ÿèƒ½ï¼š
 * - é«˜é€Ÿãƒãƒƒãƒå‡¦ç†
 * - ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†
 * - ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ»å±•é–‹
 * - ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–
 * - ã‚ªãƒ•ãƒ©ã‚¤ãƒ³åŒæœŸæœ€é©åŒ–
 */

import Dexie, { Table, Transaction } from 'dexie'
import { ref, computed, onUnmounted } from 'vue'

// æœ€é©åŒ–è¨­å®š
interface IndexedDBOptimizedConfig {
  dbName: string
  version: number
  batchSize: number
  compressionThreshold: number // ãƒã‚¤ãƒˆ
  indexCacheSize: number
  transactionTimeout: number // ãƒŸãƒªç§’
  enableCompression: boolean
  enableIndexCache: boolean
  debugMode: boolean
}

// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
interface IndexedDBMetrics {
  operationCount: number
  averageReadTime: number
  averageWriteTime: number
  cacheHitRate: number
  compressionRatio: number
  indexHitRate: number
  transactionSuccess: number
  transactionErrors: number
}

// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
const defaultConfig: IndexedDBOptimizedConfig = {
  dbName: 'HAQEIOptimizedDB',
  version: 3,
  batchSize: 100,
  compressionThreshold: 5000, // 5KB
  indexCacheSize: 1000,
  transactionTimeout: 30000, // 30ç§’
  enableCompression: true,
  enableIndexCache: true,
  debugMode: import.meta.env.DEV
}

// æœ€é©åŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©
interface OptimizedAnalysisData {
  id?: number
  userId: string
  sessionId: string
  data: string // åœ§ç¸®ãƒ‡ãƒ¼ã‚¿
  compressed: boolean
  originalSize: number
  createdAt: number // timestamp
  updatedAt: number // timestamp
  priority: 'high' | 'medium' | 'low'
  tags: string[] // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ç”¨
  syncStatus: 'pending' | 'synced' | 'error'
}

interface CacheEntry {
  id?: number
  key: string
  data: string
  timestamp: number
  ttl: number
  hits: number
}

interface IndexStats {
  id?: number
  tableName: string
  indexName: string
  totalRecords: number
  avgHitRate: number
  lastOptimized: number
}

/**
 * æœ€é©åŒ–IndexedDBã‚¯ãƒ©ã‚¹
 */
class OptimizedHAQEIDB extends Dexie {
  analyses!: Table<OptimizedAnalysisData>
  cache!: Table<CacheEntry>
  indexStats!: Table<IndexStats>

  constructor(config: IndexedDBOptimizedConfig) {
    super(config.dbName)
    
    this.version(config.version).stores({
      // æœ€é©åŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©
      analyses: '++id, userId, sessionId, createdAt, priority, syncStatus, *tags',
      cache: '++id, &key, timestamp, hits',
      indexStats: '++id, &[tableName+indexName], totalRecords, avgHitRate'
    })
    
    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ•ãƒƒã‚¯
    this.analyses.hook('creating', (primKey, obj, trans) => {
      obj.createdAt = Date.now()
      obj.updatedAt = Date.now()
      if (config.enableCompression && obj.originalSize > config.compressionThreshold) {
        obj.data = this.compressData(obj.data)
        obj.compressed = true
      }
    })
    
    this.analyses.hook('updating', (modifications, primKey, obj, trans) => {
      modifications.updatedAt = Date.now()
    })
  }
  
  /**
   * ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ï¼ˆç°¡æ˜“ç‰ˆLZåœ§ç¸®ï¼‰
   */
  compressData(data: string): string {
    try {
      // å®Ÿéš›ã®å®Ÿè£…ã§ã¯ pako ã‚„ lz-string ã‚’ä½¿ç”¨
      return btoa(encodeURIComponent(data))
    } catch (error) {
      console.warn('Data compression failed:', error)
      return data
    }
  }
  
  /**
   * ãƒ‡ãƒ¼ã‚¿å±•é–‹
   */
  decompressData(compressedData: string): string {
    try {
      return decodeURIComponent(atob(compressedData))
    } catch (error) {
      console.warn('Data decompression failed:', error)
      return compressedData
    }
  }
}

/**
 * é«˜é€ŸIndexedDBæœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ã‚¶ãƒ–ãƒ«
 */
export function useIndexedDBOptimized(config: Partial<IndexedDBOptimizedConfig> = {}) {
  const mergedConfig = { ...defaultConfig, ...config }
  
  // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
  const db = new OptimizedHAQEIDB(mergedConfig)
  
  // çŠ¶æ…‹ç®¡ç†
  const isConnected = ref(false)
  const metrics = ref<IndexedDBMetrics>({
    operationCount: 0,
    averageReadTime: 0,
    averageWriteTime: 0,
    cacheHitRate: 0,
    compressionRatio: 0,
    indexHitRate: 0,
    transactionSuccess: 0,
    transactionErrors: 0
  })
  
  // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥
  const indexCache = new Map<string, {
    data: any[]
    timestamp: number
    hits: number
  }>()
  
  // ãƒãƒƒãƒå‡¦ç†ã‚­ãƒ¥ãƒ¼
  const batchQueue = new Map<string, {
    operations: Array<{
      type: 'create' | 'update' | 'delete'
      data: any
      resolve: (value: any) => void
      reject: (error: any) => void
    }>
    timer: NodeJS.Timeout | null
  }>()
  
  /**
   * ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šåˆæœŸåŒ–
   */
  async function initializeDB(): Promise<void> {
    try {
      await db.open()
      isConnected.value = true
      
      // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆã®åˆæœŸåŒ–
      await initializeIndexStats()
      
      if (mergedConfig.debugMode) {
        console.log('âœ… Optimized IndexedDB initialized')
      }
    } catch (error) {
      console.error('âŒ Failed to initialize IndexedDB:', error)
      throw error
    }
  }
  
  /**
   * ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆåˆæœŸåŒ–
   */
  async function initializeIndexStats(): Promise<void> {
    const tables = ['analyses']
    const indexes = ['userId', 'sessionId', 'priority', 'syncStatus', 'tags']
    
    for (const tableName of tables) {
      for (const indexName of indexes) {
        const existingStats = await db.indexStats
          .where('[tableName+indexName]')
          .equals([tableName, indexName])
          .first()
        
        if (!existingStats) {
          await db.indexStats.add({
            tableName,
            indexName,
            totalRecords: 0,
            avgHitRate: 0,
            lastOptimized: Date.now()
          })
        }
      }
    }
  }
  
  /**
   * é«˜é€Ÿãƒãƒƒãƒä½œæˆ
   */
  async function batchCreate<T>(
    tableName: string,
    records: T[],
    options: { 
      priority?: 'high' | 'medium' | 'low'
      tags?: string[]
      compress?: boolean 
    } = {}
  ): Promise<number[]> {
    const startTime = performance.now()
    
    try {
      const processedRecords = records.map(record => {
        const data = JSON.stringify(record)
        const originalSize = data.length
        
        return {
          ...record,
          data: mergedConfig.enableCompression && originalSize > mergedConfig.compressionThreshold
            ? db.compressData(data)
            : data,
          compressed: mergedConfig.enableCompression && originalSize > mergedConfig.compressionThreshold,
          originalSize,
          priority: options.priority || 'medium',
          tags: options.tags || [],
          syncStatus: 'pending' as const
        }
      })
      
      const result = await db.transaction('rw', db.analyses, async () => {
        const ids: number[] = []
        
        // ãƒãƒƒãƒã‚µã‚¤ã‚ºã§åˆ†å‰²å‡¦ç†
        for (let i = 0; i < processedRecords.length; i += mergedConfig.batchSize) {
          const batch = processedRecords.slice(i, i + mergedConfig.batchSize)
          const batchIds = await db.analyses.bulkAdd(batch, { allKeys: true })
          ids.push(...batchIds as number[])
        }
        
        return ids
      })
      
      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
      updateMetrics('write', performance.now() - startTime, true)
      
      if (mergedConfig.debugMode) {
        console.log(`âœ… Batch created ${records.length} records in ${performance.now() - startTime}ms`)
      }
      
      return result
      
    } catch (error) {
      updateMetrics('write', performance.now() - startTime, false)
      console.error('âŒ Batch create failed:', error)
      throw error
    }
  }
  
  /**
   * é«˜é€Ÿã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢
   */
  async function fastIndexSearch<T>(
    tableName: string,
    indexName: string,
    value: any,
    options: {
      useCache?: boolean
      limit?: number
      offset?: number
    } = {}
  ): Promise<T[]> {
    const startTime = performance.now()
    const cacheKey = `${tableName}_${indexName}_${JSON.stringify(value)}`
    
    try {
      // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
      if (options.useCache !== false && mergedConfig.enableIndexCache) {
        const cached = indexCache.get(cacheKey)
        if (cached && Date.now() - cached.timestamp < 300000) { // 5åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
          cached.hits++
          updateIndexCacheStats(true)
          return cached.data.slice(options.offset || 0, (options.offset || 0) + (options.limit || cached.data.length))
        }
      }
      
      // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
      let query = db.analyses.where(indexName).equals(value)
      
      if (options.offset) {
        query = query.offset(options.offset)
      }
      
      if (options.limit) {
        query = query.limit(options.limit)
      }
      
      const results = await query.toArray()
      
      // ãƒ‡ãƒ¼ã‚¿å±•é–‹
      const processedResults = results.map(record => {
        if (record.compressed) {
          const decompressedData = db.decompressData(record.data)
          return { ...JSON.parse(decompressedData), id: record.id }
        }
        return { ...JSON.parse(record.data), id: record.id }
      }) as T[]
      
      // ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
      if (mergedConfig.enableIndexCache && indexCache.size < mergedConfig.indexCacheSize) {
        indexCache.set(cacheKey, {
          data: processedResults,
          timestamp: Date.now(),
          hits: 1
        })
      }
      
      // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆæ›´æ–°
      await updateIndexStats(tableName, indexName, results.length > 0)
      
      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
      updateMetrics('read', performance.now() - startTime, true)
      updateIndexCacheStats(false)
      
      return processedResults
      
    } catch (error) {
      updateMetrics('read', performance.now() - startTime, false)
      console.error('âŒ Fast index search failed:', error)
      throw error
    }
  }
  
  /**
   * é«˜é€Ÿãƒãƒƒãƒæ›´æ–°
   */
  async function batchUpdate(
    updates: Array<{ id: number; data: any }>
  ): Promise<void> {
    const startTime = performance.now()
    
    try {
      await db.transaction('rw', db.analyses, async () => {
        // ãƒãƒƒãƒã‚µã‚¤ã‚ºã§åˆ†å‰²å‡¦ç†
        for (let i = 0; i < updates.length; i += mergedConfig.batchSize) {
          const batch = updates.slice(i, i + mergedConfig.batchSize)
          
          await Promise.all(
            batch.map(async ({ id, data }) => {
              const jsonData = JSON.stringify(data)
              const originalSize = jsonData.length
              
              const updateData = {
                data: mergedConfig.enableCompression && originalSize > mergedConfig.compressionThreshold
                  ? db.compressData(jsonData)
                  : jsonData,
                compressed: mergedConfig.enableCompression && originalSize > mergedConfig.compressionThreshold,
                originalSize,
                updatedAt: Date.now()
              }
              
              return db.analyses.update(id, updateData)
            })
          )
        }
      })
      
      // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
      indexCache.clear()
      
      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
      updateMetrics('write', performance.now() - startTime, true)
      
      if (mergedConfig.debugMode) {
        console.log(`âœ… Batch updated ${updates.length} records in ${performance.now() - startTime}ms`)
      }
      
    } catch (error) {
      updateMetrics('write', performance.now() - startTime, false)
      console.error('âŒ Batch update failed:', error)
      throw error
    }
  }
  
  /**
   * é«˜é€Ÿå‰Šé™¤
   */
  async function fastDelete(ids: number[]): Promise<void> {
    const startTime = performance.now()
    
    try {
      await db.transaction('rw', db.analyses, async () => {
        // ãƒãƒƒãƒã‚µã‚¤ã‚ºã§åˆ†å‰²å‡¦ç†
        for (let i = 0; i < ids.length; i += mergedConfig.batchSize) {
          const batch = ids.slice(i, i + mergedConfig.batchSize)
          await db.analyses.bulkDelete(batch)
        }
      })
      
      // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
      indexCache.clear()
      
      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
      updateMetrics('write', performance.now() - startTime, true)
      
      if (mergedConfig.debugMode) {
        console.log(`âœ… Fast deleted ${ids.length} records in ${performance.now() - startTime}ms`)
      }
      
    } catch (error) {
      updateMetrics('write', performance.now() - startTime, false)
      console.error('âŒ Fast delete failed:', error)
      throw error
    }
  }
  
  /**
   * å…¨æ–‡æ¤œç´¢ï¼ˆã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ï¼‰
   */
  async function fullTextSearch<T>(
    searchTerm: string,
    options: {
      limit?: number
      fuzzy?: boolean
    } = {}
  ): Promise<T[]> {
    const startTime = performance.now()
    
    try {
      const searchTerms = searchTerm.toLowerCase().split(/\s+/)
      
      const results = await db.analyses
        .where('tags')
        .anyOf(searchTerms)
        .limit(options.limit || 100)
        .toArray()
      
      // ãƒ‡ãƒ¼ã‚¿å±•é–‹ã¨ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
      const processedResults = results
        .map(record => {
          const data = record.compressed 
            ? JSON.parse(db.decompressData(record.data))
            : JSON.parse(record.data)
          
          // ç°¡æ˜“ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
          const score = record.tags.filter(tag => 
            searchTerms.some(term => tag.toLowerCase().includes(term))
          ).length
          
          return { ...data, id: record.id, _score: score }
        })
        .sort((a, b) => b._score - a._score) as T[]
      
      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
      updateMetrics('read', performance.now() - startTime, true)
      
      return processedResults
      
    } catch (error) {
      updateMetrics('read', performance.now() - startTime, false)
      console.error('âŒ Full text search failed:', error)
      throw error
    }
  }
  
  /**
   * ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆå–å¾—
   */
  async function getDataStatistics(): Promise<{
    totalRecords: number
    totalSize: number
    compressionRatio: number
    avgRecordSize: number
    tableStats: Array<{ name: string; count: number; size: number }>
  }> {
    try {
      const analysesCount = await db.analyses.count()
      const analysesRecords = await db.analyses.toArray()
      
      const totalSize = analysesRecords.reduce((sum, record) => sum + record.originalSize, 0)
      const compressedSize = analysesRecords.reduce((sum, record) => sum + record.data.length, 0)
      
      return {
        totalRecords: analysesCount,
        totalSize,
        compressionRatio: totalSize > 0 ? Math.round((1 - compressedSize / totalSize) * 100) : 0,
        avgRecordSize: analysesCount > 0 ? Math.round(totalSize / analysesCount) : 0,
        tableStats: [
          { name: 'analyses', count: analysesCount, size: totalSize }
        ]
      }
    } catch (error) {
      console.error('âŒ Failed to get data statistics:', error)
      throw error
    }
  }
  
  /**
   * ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
   */
  function updateMetrics(operation: 'read' | 'write', duration: number, success: boolean) {
    metrics.value.operationCount++
    
    if (success) {
      if (operation === 'read') {
        const alpha = 0.1
        metrics.value.averageReadTime = (1 - alpha) * metrics.value.averageReadTime + alpha * duration
      } else {
        const alpha = 0.1
        metrics.value.averageWriteTime = (1 - alpha) * metrics.value.averageWriteTime + alpha * duration
      }
      metrics.value.transactionSuccess++
    } else {
      metrics.value.transactionErrors++
    }
  }
  
  /**
   * ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæ›´æ–°
   */
  function updateIndexCacheStats(hit: boolean) {
    const totalCacheOps = metrics.value.cacheHitRate * (metrics.value.operationCount - 1) + (hit ? 1 : 0)
    metrics.value.cacheHitRate = totalCacheOps / metrics.value.operationCount
  }
  
  /**
   * ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆæ›´æ–°
   */
  async function updateIndexStats(tableName: string, indexName: string, hit: boolean) {
    try {
      const stats = await db.indexStats
        .where('[tableName+indexName]')
        .equals([tableName, indexName])
        .first()
      
      if (stats) {
        const newHitRate = (stats.avgHitRate * stats.totalRecords + (hit ? 1 : 0)) / (stats.totalRecords + 1)
        
        await db.indexStats.update(stats.id!, {
          avgHitRate: newHitRate,
          totalRecords: stats.totalRecords + 1
        })
      }
    } catch (error) {
      console.warn('Failed to update index stats:', error)
    }
  }
  
  /**
   * ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–
   */
  async function optimizeIndexes(): Promise<void> {
    try {
      // ä½¿ç”¨é »åº¦ã®ä½ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
      const cacheEntries = Array.from(indexCache.entries())
      const sortedEntries = cacheEntries.sort((a, b) => a[1].hits - b[1].hits)
      
      // ä¸‹ä½30%ã‚’å‰Šé™¤
      const deleteCount = Math.floor(sortedEntries.length * 0.3)
      for (let i = 0; i < deleteCount; i++) {
        indexCache.delete(sortedEntries[i][0])
      }
      
      if (mergedConfig.debugMode) {
        console.log(`ğŸ”§ Optimized indexes: removed ${deleteCount} cache entries`)
      }
    } catch (error) {
      console.error('âŒ Index optimization failed:', error)
    }
  }
  
  /**
   * ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
   */
  async function cleanup(): Promise<void> {
    try {
      // å¤ã„ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤ï¼ˆ30æ—¥ä»¥ä¸Šï¼‰
      const thirtyDaysAgo = Date.now() - (30 * 24 * 60 * 60 * 1000)
      await db.analyses.where('createdAt').below(thirtyDaysAgo).delete()
      
      // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
      indexCache.clear()
      
      if (mergedConfig.debugMode) {
        console.log('ğŸ§¹ Database cleanup completed')
      }
    } catch (error) {
      console.error('âŒ Database cleanup failed:', error)
    }
  }
  
  // è¨ˆç®—ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
  const averageOperationTime = computed(() => {
    return (metrics.value.averageReadTime + metrics.value.averageWriteTime) / 2
  })
  
  const successRate = computed(() => {
    const total = metrics.value.transactionSuccess + metrics.value.transactionErrors
    return total > 0 ? (metrics.value.transactionSuccess / total) * 100 : 0
  })
  
  const cacheEfficiency = computed(() => {
    return metrics.value.cacheHitRate * 100
  })
  
  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  onUnmounted(() => {
    if (db.isOpen()) {
      db.close()
    }
    
    // ãƒãƒƒãƒã‚¿ã‚¤ãƒãƒ¼ã‚¯ãƒªã‚¢
    for (const batch of batchQueue.values()) {
      if (batch.timer) {
        clearTimeout(batch.timer)
      }
    }
    batchQueue.clear()
    
    indexCache.clear()
  })
  
  return {
    // çŠ¶æ…‹
    isConnected,
    metrics,
    
    // è¨ˆç®—ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    averageOperationTime,
    successRate,
    cacheEfficiency,
    
    // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ
    initializeDB,
    batchCreate,
    fastIndexSearch,
    batchUpdate,
    fastDelete,
    fullTextSearch,
    
    // çµ±è¨ˆãƒ»æœ€é©åŒ–
    getDataStatistics,
    optimizeIndexes,
    cleanup,
    
    // ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰
    db
  }
}

export type { IndexedDBOptimizedConfig, IndexedDBMetrics, OptimizedAnalysisData }