# ğŸ“ 384çˆ»ã‚·ã‚¹ãƒ†ãƒ  Cloudflare Pageså¯¾å¿œ ã‚¿ã‚¹ã‚¯åˆ†è§£æ›¸

**æ–‡æ›¸ç•ªå·**: TD-384-004  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 4.0ï¼ˆCloudflare Pageså¯¾å¿œç‰ˆï¼‰  
**ä½œæˆæ—¥**: 2025å¹´8æœˆ28æ—¥  
**ä½œæˆè€…**: HAQEIé–‹ç™ºãƒãƒ¼ãƒ   
**æ‰¿èªè€…**: [æœªæ‰¿èª]  

---

## 1. ã‚¿ã‚¹ã‚¯åˆ†è§£æ¦‚è¦

### 1.1 WBSï¼ˆWork Breakdown Structureï¼‰

```
384çˆ»ã‚·ã‚¹ãƒ†ãƒ  ç¾å®Ÿçš„Cloudflare Pageså®Ÿè£…ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
â”œâ”€ Phase 1: åŸºç›¤æ§‹ç¯‰ï¼ˆ2é€±é–“ï¼‰
â”‚   â”œâ”€ Task 1.1: Cloudflare Pagesç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
â”‚   â”œâ”€ Task 1.2: D1 DatabaseåˆæœŸåŒ–ãƒ»koudo_shishin.jsonçµ±åˆ
â”‚   â”œâ”€ Task 1.3: Workers KVè¨­å®šãƒ»åŸºæœ¬å‹•ä½œç¢ºèª
â”‚   â””â”€ Task 1.4: IndexedDBåŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…
â”œâ”€ Phase 2: æ±ºå®šè«–çš„åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆ2é€±é–“ï¼‰
â”‚   â”œâ”€ Task 2.1: åŸºæœ¬ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè£…
â”‚   â”œâ”€ Task 2.2: æ±ºå®šè«–çš„ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…
â”‚   â”œâ”€ Task 2.3: 384çˆ»ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒãƒ³ã‚°æ©Ÿèƒ½
â”‚   â””â”€ Task 2.4: ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
â”œâ”€ Phase 3: å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ2é€±é–“ï¼‰
â”‚   â”œâ”€ Task 3.1: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ã‚·ã‚¹ãƒ†ãƒ 
â”‚   â”œâ”€ Task 3.2: æ—¥æ¬¡ãƒãƒƒãƒå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
â”‚   â”œâ”€ Task 3.3: ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»æ±ºå®šè«–æ€§ä¿è¨¼
â”‚   â””â”€ Task 3.4: å­¦ç¿’çµ±è¨ˆãƒ»ãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½
â””â”€ Phase 4: çµ±åˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆ1é€±é–“ï¼‰
    â”œâ”€ Task 4.1: çµ±åˆãƒ†ã‚¹ãƒˆãƒ»æ±ºå®šè«–æ€§ç¢ºèª
    â”œâ”€ Task 4.2: æ€§èƒ½ãƒ†ã‚¹ãƒˆãƒ»æœ€é©åŒ–
    â”œâ”€ Task 4.3: Cloudflare Pagesæœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤
    â””â”€ Task 4.4: é‹ç”¨ç›£è¦–ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
```

### 1.2 ç¾å®Ÿçš„å·¥æ•°è¦‹ç©ã‚‚ã‚Š
- **ç·å·¥æ•°**: 7é€±é–“ï¼ˆ35å–¶æ¥­æ—¥ï¼‰
- **å¿…è¦äººå“¡**: 2åï¼ˆãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯é–‹ç™ºè€…2åï¼‰
- **å‰ææ¡ä»¶**: JavaScriptã€CloudflareåŸºæœ¬çŸ¥è­˜ã€æ±ºå®šè«–çš„ã‚·ã‚¹ãƒ†ãƒ ç†è§£

---

## 2. Phase 1: Edgeé–‹ç™ºç’°å¢ƒæ§‹ç¯‰ï¼ˆ1é€±é–“ï¼‰

### Task 1.1: Cloudflare Pagesç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆ2æ—¥ï¼‰

| ID | ä½œæ¥­é …ç›® | å®Ÿè£…å†…å®¹ | å®Œäº†æ¡ä»¶ |
|----|---------|---------|----------|
| 1.1.1 | Cloudflare Pagesè¨­å®š | GitHubé€£æºãƒ»è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®š | ãƒ“ãƒ«ãƒ‰ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤å‹•ä½œç¢ºèª |
| 1.1.2 | Workersè¨­å®š | Functions/_middleware.tsè¨­å®š | Edgeå‡¦ç†å‹•ä½œç¢ºèª |
| 1.1.3 | Custom Domainè¨­å®š | DNSãƒ»SSLè¨¼æ˜æ›¸è¨­å®š | HTTPSæ¥ç¶šç¢ºèª |
| 1.1.4 | ç’°å¢ƒå¤‰æ•°ãƒ»Secretsè¨­å®š | D1ãƒ»KVæ¥ç¶šæƒ…å ±è¨­å®š | èªè¨¼å‹•ä½œç¢ºèª |

**å…·ä½“çš„ãªå®Ÿè£…ä½œæ¥­**:
```bash
# Cloudflare CLI setup
npm install -g wrangler
wrangler login

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
wrangler pages project create haqei-analyzer
wrangler pages deployment list

# Functionsè¨­å®š
mkdir -p functions
cat > functions/_middleware.ts << 'EOF'
export async function onRequest(context) {
  // Edge middleware setup
  const response = await context.next();
  
  // Security headers
  response.headers.set('X-Frame-Options', 'DENY');
  response.headers.set('X-Content-Type-Options', 'nosniff');
  
  return response;
}
EOF

# ç’°å¢ƒå¤‰æ•°è¨­å®š
wrangler pages secret put D1_DATABASE_ID
wrangler pages secret put KV_NAMESPACE_ID
```

### Task 1.2: D1 DatabaseåˆæœŸåŒ–ãƒ»æ§‹ç¯‰ï¼ˆ2æ—¥ï¼‰

| ID | ä½œæ¥­é …ç›® | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ | å®¹é‡åˆ¶é™ | å‡¦ç†æ™‚é–“ |
|----|---------|------------|----------|----------|
| 1.2.1 | D1 Databaseä½œæˆ | Cloudflare Dashboard | 50MB | 30åˆ† |
| 1.2.2 | 384çˆ»çµ±åˆãƒ†ãƒ¼ãƒ–ãƒ«æ§‹ç¯‰ | 4ã¤ã®JSONãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆ | 30MB | 2æ™‚é–“ |
| 1.2.3 | å­¦ç¿’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ« | Edgeç”¨è»½é‡ã‚¹ã‚­ãƒ¼ãƒ | 10MB | 1æ™‚é–“ |
| 1.2.4 | ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ã‚¤ãƒ³ãƒãƒ¼ãƒˆ | JSON â†’ D1 migration | - | 3æ™‚é–“ |

**D1ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**:
```typescript
// D1 DatabaseåˆæœŸåŒ–
interface Env {
  DB: D1Database;
  KV: KVNamespace;
}

export class D1DatabaseManager {
  constructor(private env: Env) {}
  
  async initializeDatabase(): Promise<void> {
    // 384çˆ»çµ±åˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    await this.env.DB.exec(`
      CREATE TABLE IF NOT EXISTS lines_384 (
        line_id INTEGER PRIMARY KEY,
        hexagram_id INTEGER NOT NULL,
        line_position INTEGER NOT NULL,
        hexagram_name TEXT NOT NULL,
        line_name TEXT NOT NULL,
        
        -- enhanced_hexagrams_complete.json ãƒ‡ãƒ¼ã‚¿
        yaoci_text TEXT,
        yaoci_meaning TEXT,
        personality_trait TEXT,
        transformation_potential TEXT,
        
        -- yaoci_31-63.json ãƒ‡ãƒ¼ã‚¿  
        extended_yaoci TEXT,
        classical_interpretation TEXT,
        
        -- h384.json ãƒ‡ãƒ¼ã‚¿
        basic_meaning TEXT,
        keywords TEXT, -- JSONé…åˆ—æ–‡å­—åˆ—
        category TEXT,
        
        -- koudo_shishin.json ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¢å­˜äº’æ›æ€§ï¼‰
        shin_data TEXT,
        hen_data TEXT,
        
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
      );
    `);
    
    // ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    await this.env.DB.exec(`
      CREATE INDEX IF NOT EXISTS idx_hexagram_position 
      ON lines_384(hexagram_id, line_position);
      
      CREATE INDEX IF NOT EXISTS idx_keywords_search 
      ON lines_384(keywords);
    `);
    
    // å­¦ç¿’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«
    await this.env.DB.exec(`
      CREATE TABLE IF NOT EXISTS user_feedback (
        id INTEGER PRIMARY KEY,
        session_id TEXT NOT NULL,
        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
        input_text TEXT NOT NULL,
        predicted_line_id INTEGER,
        confidence_score REAL,
        correct_line_id INTEGER,
        accuracy_rating INTEGER,
        feedback_comment TEXT
      );
    `);
  }
  
  async importJSONData(): Promise<void> {
    // enhanced_hexagrams_complete.jsonçµ±åˆ
    const enhancedHexagrams = await this.loadJSON('/data/enhanced_hexagrams_complete.json');
    for (const hexagram of enhancedHexagrams) {
      for (let position = 1; position <= 6; position++) {
        const line = hexagram.six_lines[position - 1];
        const lineId = (hexagram.number - 1) * 6 + position;
        
        await this.env.DB.prepare(`
          INSERT OR REPLACE INTO lines_384 (
            line_id, hexagram_id, line_position, hexagram_name, line_name,
            yaoci_text, yaoci_meaning, personality_trait, transformation_potential
          ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        `).bind(
          lineId, hexagram.number, position, hexagram.name,
          line.name || `${this.getPositionName(position)}`,
          line.text, line.meaning, line.personality_trait, line.transformation_potential
        ).run();
      }
    }
    
    console.log('âœ… D1 DatabaseåˆæœŸåŒ–å®Œäº†');
  }
}

### Task 1.3: åŸºæœ¬æ¥ç¶šãƒ»ãƒ†ã‚¹ãƒˆç’°å¢ƒï¼ˆ1.5æ—¥ï¼‰

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼å®Ÿè£…**:
```javascript
// DatabaseManager.js
class DatabaseManager {
    constructor() {
        this.connections = {
            sqlite: null,
            mongodb: null,
            redis: null,
            influxdb: null
        };
    }
    
    async initializeSQLite() {
        const sqlite3 = require('sqlite3');
        const path = require('path');
        
        this.connections.sqlite = new sqlite3.Database(
            path.join(__dirname, '../databases/sqlite/main.db'),
            sqlite3.OPEN_READWRITE | sqlite3.OPEN_CREATE,
            (err) => {
                if (err) throw new Error(`SQLite connection failed: ${err.message}`);
                console.log('âœ… SQLite connection established');
            }
        );
        
        // æš—å·åŒ–è¨­å®š
        this.connections.sqlite.run("PRAGMA key = 'haqei-encryption-key'");
    }
    
    async initializeMongoDB() {
        const { MongoClient } = require('mongodb');
        const client = new MongoClient('mongodb://localhost:27017', {
            useUnifiedTopology: true,
            authSource: 'admin',
            authMechanism: 'SCRAM-SHA-256'
        });
        
        await client.connect();
        this.connections.mongodb = client.db('haqei_analyzer');
        console.log('âœ… MongoDB connection established');
    }
    
    async initializeRedis() {
        const redis = require('redis');
        this.connections.redis = redis.createClient({
            host: 'localhost',
            port: 6379,
            password: 'haqei-secure-password',
            db: 0
        });
        
        await this.connections.redis.connect();
        console.log('âœ… Redis connection established');
    }
    
    async healthCheck() {
        const results = {};
        
        // SQLiteå¥åº·æ€§ãƒã‚§ãƒƒã‚¯
        try {
            await new Promise((resolve, reject) => {
                this.connections.sqlite.get("SELECT 1", (err, row) => {
                    err ? reject(err) : resolve(row);
                });
            });
            results.sqlite = 'âœ… OK';
        } catch (error) {
            results.sqlite = `âŒ ERROR: ${error.message}`;
        }
        
        // MongoDBå¥åº·æ€§ãƒã‚§ãƒƒã‚¯
        try {
            await this.connections.mongodb.admin().ping();
            results.mongodb = 'âœ… OK';
        } catch (error) {
            results.mongodb = `âŒ ERROR: ${error.message}`;
        }
        
        // Rediså¥åº·æ€§ãƒã‚§ãƒƒã‚¯
        try {
            await this.connections.redis.ping();
            results.redis = 'âœ… OK';
        } catch (error) {
            results.redis = `âŒ ERROR: ${error.message}`;
        }
        
        return results;
    }
}
```

---

## 3. Phase 2: SQLiteè¾æ›¸ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ï¼ˆ2é€±é–“ï¼‰

### Task 2.1: å½¢æ…‹ç´ è§£æè¾æ›¸ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ï¼ˆ4æ—¥ï¼‰

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒå®Ÿè£…**:
```sql
-- morphology.db
CREATE TABLE morphology_dict (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    word TEXT NOT NULL,
    reading TEXT,
    part_of_speech TEXT NOT NULL,
    semantic_category TEXT,
    frequency INTEGER DEFAULT 1,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_word_lookup ON morphology_dict(word);
CREATE INDEX idx_pos_category ON morphology_dict(part_of_speech, semantic_category);

CREATE TABLE specialized_terms (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    term TEXT NOT NULL UNIQUE,
    domain TEXT NOT NULL,  -- 'yijing', 'philosophy', etc.
    weight REAL DEFAULT 1.0,
    synonyms TEXT,  -- JSON array
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_term_domain ON specialized_terms(term, domain);
```

**MeCabçµ±åˆã‚¯ãƒ©ã‚¹å®Ÿè£…**:
```javascript
class MeCabAnalyzer {
    constructor(databaseManager) {
        this.mecab = require('mecab-async');
        this.db = databaseManager.connections.sqlite;
    }
    
    async analyze(text) {
        // MeCabå½¢æ…‹ç´ è§£æå®Ÿè¡Œ
        const result = await this.mecab.parse(text);
        const tokens = result.map(token => ({
            surface: token[0],
            pos: token[1],
            pos_detail: token[2],
            reading: token[7],
            base_form: token[6]
        }));
        
        // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰è¿½åŠ æƒ…å ±å–å¾—
        const enrichedTokens = [];
        for (const token of tokens) {
            const dbInfo = await this.queryMorphologyDB(token.base_form);
            enrichedTokens.push({
                ...token,
                semantic_category: dbInfo?.semantic_category,
                frequency: dbInfo?.frequency || 1,
                is_specialized: await this.isSpecializedTerm(token.base_form)
            });
        }
        
        return {
            tokens: enrichedTokens,
            keywords: this.extractKeywords(enrichedTokens),
            semantic_analysis: this.analyzeSemantics(enrichedTokens)
        };
    }
    
    async queryMorphologyDB(word) {
        return new Promise((resolve, reject) => {
            this.db.get(
                "SELECT * FROM morphology_dict WHERE word = ?",
                [word],
                (err, row) => {
                    if (err) reject(err);
                    else resolve(row);
                }
            );
        });
    }
    
    extractKeywords(tokens) {
        return tokens
            .filter(token => ['åè©', 'å‹•è©', 'å½¢å®¹è©'].includes(token.pos))
            .filter(token => token.surface.length > 1)
            .map(token => ({
                word: token.base_form || token.surface,
                pos: token.pos,
                weight: this.calculateWeight(token),
                semantic_category: token.semantic_category
            }))
            .sort((a, b) => b.weight - a.weight);
    }
    
    calculateWeight(token) {
        let weight = 1.0;
        
        // å“è©ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        if (token.pos === 'åè©') weight *= 1.2;
        if (token.pos === 'å‹•è©') weight *= 1.1;
        if (token.pos === 'å½¢å®¹è©') weight *= 1.0;
        
        // é »åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘ï¼ˆé€†ç›¸é–¢ï¼‰
        if (token.frequency) {
            weight *= Math.log(10000 / token.frequency);
        }
        
        // å°‚é–€ç”¨èªã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        if (token.is_specialized) weight *= 1.5;
        
        return weight;
    }
}
```

### Task 2.2: é¡ç¾©èªè¾æ›¸ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ï¼ˆ4æ—¥ï¼‰

**WordNetçµ±åˆã‚¯ãƒ©ã‚¹å®Ÿè£…**:
```javascript
class WordNetSynonymMatcher {
    constructor(databaseManager) {
        this.db = databaseManager.connections.sqlite;
        this.wordnet = require('wordnet');  // æ—¥æœ¬èªWordNetãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    }
    
    async findSynonyms(word, threshold = 0.7) {
        // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        const cached = await this.getCachedSynonyms(word);
        if (cached.length > 0) return cached;
        
        // WordNetã‹ã‚‰é¡ç¾©èªæ¤œç´¢
        const synsets = await this.wordnet.lookup(word);
        const synonyms = [];
        
        for (const synset of synsets) {
            // åŒã˜ã‚·ãƒ³ã‚»ãƒƒãƒˆå†…ã®èªå½™
            for (const synonym of synset.words) {
                if (synonym !== word) {
                    synonyms.push({
                        word: synonym,
                        similarity: 1.0,
                        relation: 'synonym',
                        source: 'wordnet'
                    });
                }
            }
            
            // ä¸Šä½æ¦‚å¿µï¼ˆhypernymï¼‰
            for (const hypernym of synset.hypernyms()) {
                for (const word of hypernym.words) {
                    synonyms.push({
                        word: word,
                        similarity: 0.8,
                        relation: 'hypernym',
                        source: 'wordnet'
                    });
                }
            }
            
            // ä¸‹ä½æ¦‚å¿µï¼ˆhyponymï¼‰
            for (const hyponym of synset.hyponyms()) {
                for (const word of hyponym.words) {
                    synonyms.push({
                        word: word,
                        similarity: 0.7,
                        relation: 'hyponym',
                        source: 'wordnet'
                    });
                }
            }
        }
        
        // é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»é‡è¤‡é™¤å»
        const filtered = synonyms
            .filter(syn => syn.similarity >= threshold)
            .reduce((acc, current) => {
                const exists = acc.find(item => item.word === current.word);
                if (!exists || exists.similarity < current.similarity) {
                    return [...acc.filter(item => item.word !== current.word), current];
                }
                return acc;
            }, []);
        
        // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        await this.cacheSynonyms(word, filtered);
        
        return filtered;
    }
    
    async getCachedSynonyms(word) {
        return new Promise((resolve, reject) => {
            this.db.all(`
                SELECT word2, similarity_score, relation_type 
                FROM synonym_relations 
                WHERE word1 = ? AND similarity_score >= 0.7
                ORDER BY similarity_score DESC
            `, [word], (err, rows) => {
                if (err) reject(err);
                else resolve(rows.map(row => ({
                    word: row.word2,
                    similarity: row.similarity_score,
                    relation: row.relation_type,
                    source: 'cache'
                })));
            });
        });
    }
    
    async calculateSemanticSimilarity(word1, word2) {
        // Wu-Palmeré¡ä¼¼åº¦è¨ˆç®—
        const synsets1 = await this.wordnet.lookup(word1);
        const synsets2 = await this.wordnet.lookup(word2);
        
        let maxSimilarity = 0;
        
        for (const synset1 of synsets1) {
            for (const synset2 of synsets2) {
                const similarity = await this.wupSimilarity(synset1, synset2);
                maxSimilarity = Math.max(maxSimilarity, similarity);
            }
        }
        
        return maxSimilarity;
    }
}
```

### Task 2.3: 384çˆ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚·ã‚¹ãƒ†ãƒ çµ±åˆï¼ˆ3æ—¥ï¼‰

**çˆ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ **:
```javascript
class LineKeywordManager {
    constructor(databaseManager) {
        this.db = databaseManager.connections.sqlite;
    }
    
    async buildKeywordDatabase() {
        // æ—¢å­˜JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        const enhancedHexagrams = await this.loadEnhancedHexagrams();
        const koudoShishin = await this.loadKoudoShishin();
        
        for (let lineId = 1; lineId <= 384; lineId++) {
            const hexagramId = Math.ceil(lineId / 6);
            const position = ((lineId - 1) % 6) + 1;
            
            // enhanced_hexagrams_complete.jsonã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            const hexagram = enhancedHexagrams.find(h => h.hexagram_id === hexagramId);
            if (hexagram && hexagram.six_lines[position - 1]) {
                const lineData = hexagram.six_lines[position - 1];
                await this.extractAndStoreKeywords(lineId, lineData.text, 'yaoci', 'enhanced_hexagrams');
                await this.extractAndStoreKeywords(lineId, lineData.meaning, 'meaning', 'enhanced_hexagrams');
                await this.extractAndStoreKeywords(lineId, lineData.personality_trait, 'personality', 'enhanced_hexagrams');
            }
            
            // koudo_shishin.jsonã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            const koudoData = koudoShishin[lineId - 1];
            if (koudoData) {
                await this.extractAndStoreKeywords(lineId, koudoData.shin, 'shin', 'koudo_shishin');
                await this.extractAndStoreKeywords(lineId, koudoData.hen, 'hen', 'koudo_shishin');
            }
        }
    }
    
    async extractAndStoreKeywords(lineId, text, category, source) {
        if (!text) return;
        
        // MeCabè§£æã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        const analyzer = new MeCabAnalyzer(this.db);
        const analysis = await analyzer.analyze(text);
        
        for (const keyword of analysis.keywords) {
            // é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ»é‡ã¿è¨ˆç®—
            const weight = await this.calculateKeywordWeight(keyword, category);
            
            await this.storeKeyword(lineId, keyword.word, weight, category, source);
        }
    }
    
    async storeKeyword(lineId, keyword, weight, category, source) {
        return new Promise((resolve, reject) => {
            this.db.run(`
                INSERT OR REPLACE INTO line_keywords 
                (line_id, keyword, weight, category, source, updated_at) 
                VALUES (?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            `, [lineId, keyword, weight, category, source], (err) => {
                if (err) reject(err);
                else resolve();
            });
        });
    }
    
    async getLineKeywords(lineId, category = null) {
        const whereClause = category ? 'WHERE line_id = ? AND category = ?' : 'WHERE line_id = ?';
        const params = category ? [lineId, category] : [lineId];
        
        return new Promise((resolve, reject) => {
            this.db.all(`
                SELECT keyword, weight, category, source 
                FROM line_keywords ${whereClause}
                ORDER BY weight DESC
            `, params, (err, rows) => {
                if (err) reject(err);
                else resolve(rows);
            });
        });
    }
}
```

---

## 4. Phase 3: MongoDBå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ï¼ˆ2é€±é–“ï¼‰

### Task 3.1: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ4æ—¥ï¼‰

**ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ **:
```javascript
class FeedbackManager {
    constructor(databaseManager) {
        this.mongodb = databaseManager.connections.mongodb;
        this.collection = this.mongodb.collection('user_feedback');
    }
    
    async recordFeedback(sessionId, userInput, analysisResult, userFeedback) {
        const feedbackDocument = {
            timestamp: new Date(),
            session_id: sessionId,
            user_input: {
                text: userInput.text,
                length: userInput.text.length,
                language: await this.detectLanguage(userInput.text)
            },
            analysis_result: {
                predicted_line_id: analysisResult.lineId,
                confidence_score: analysisResult.confidence,
                processing_time_ms: analysisResult.processingTime,
                morphology_tokens: analysisResult.morphologyTokens,
                extracted_keywords: analysisResult.keywords,
                similarity_scores: analysisResult.allScores
            },
            user_feedback: {
                rating: userFeedback.rating,
                correct_line_id: userFeedback.correctLineId,
                feedback_type: userFeedback.type,
                comments: userFeedback.comments
            },
            metadata: {
                user_agent: userInput.userAgent,
                ip_address: userInput.ipAddress,
                feature_flags: userInput.featureFlags
            }
        };
        
        const result = await this.collection.insertOne(feedbackDocument);
        
        // å³åº§ã«å­¦ç¿’çµ±è¨ˆæ›´æ–°
        await this.updateLearningStatistics(analysisResult.lineId, userFeedback);
        
        return result.insertedId;
    }
    
    async updateLearningStatistics(lineId, userFeedback) {
        const statsCollection = this.mongodb.collection('learning_statistics');
        const today = new Date().toISOString().split('T')[0];
        
        // ä»Šæ—¥ã®çµ±è¨ˆå–å¾—ã¾ãŸã¯åˆæœŸåŒ–
        let stats = await statsCollection.findOne({ line_id: lineId, date: today });
        if (!stats) {
            stats = {
                line_id: lineId,
                date: today,
                statistics: {
                    total_predictions: 0,
                    correct_predictions: 0,
                    accuracy_rate: 0,
                    avg_confidence: 0,
                    common_mistakes: [],
                    improvement_trend: []
                },
                keyword_performance: {
                    effective_keywords: {},
                    ineffective_keywords: {},
                    new_keywords: []
                }
            };
        }
        
        // çµ±è¨ˆæ›´æ–°
        stats.statistics.total_predictions++;
        if (userFeedback.rating >= 4) {
            stats.statistics.correct_predictions++;
        }
        stats.statistics.accuracy_rate = stats.statistics.correct_predictions / stats.statistics.total_predictions;
        
        // é–“é•ã„ãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜éŒ²
        if (userFeedback.correctLineId && userFeedback.correctLineId !== lineId) {
            const mistake = stats.statistics.common_mistakes.find(m => m.confused_with === userFeedback.correctLineId);
            if (mistake) {
                mistake.frequency++;
            } else {
                stats.statistics.common_mistakes.push({
                    confused_with: userFeedback.correctLineId,
                    frequency: 1
                });
            }
        }
        
        // çµ±è¨ˆä¿å­˜
        await statsCollection.replaceOne(
            { line_id: lineId, date: today },
            stats,
            { upsert: true }
        );
    }
}
```

### Task 3.2: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»çµ±è¨ˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆ4æ—¥ï¼‰

**å­¦ç¿’çµ±è¨ˆåˆ†æã‚·ã‚¹ãƒ†ãƒ **:
```javascript
class LearningAnalytics {
    constructor(databaseManager) {
        this.mongodb = databaseManager.connections.mongodb;
    }
    
    async generateDailyReport(date = null) {
        const targetDate = date || new Date().toISOString().split('T')[0];
        
        const pipeline = [
            { $match: { date: targetDate } },
            { $group: {
                _id: null,
                total_lines_analyzed: { $sum: "$statistics.total_predictions" },
                total_correct: { $sum: "$statistics.correct_predictions" },
                avg_accuracy: { $avg: "$statistics.accuracy_rate" },
                lines_with_improvement: { $sum: { $cond: [{ $gt: ["$statistics.accuracy_rate", 0.7] }, 1, 0] } }
            }},
            { $addFields: {
                overall_accuracy: { $divide: ["$total_correct", "$total_lines_analyzed"] },
                improvement_percentage: { $multiply: [{ $divide: ["$lines_with_improvement", 384] }, 100] }
            }}
        ];
        
        const result = await this.mongodb.collection('learning_statistics').aggregate(pipeline).toArray();
        return result[0] || null;
    }
    
    async identifyProblematicLines(threshold = 0.5) {
        const pipeline = [
            { $match: { 
                "statistics.total_predictions": { $gte: 10 },  // ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«æ•°
                "statistics.accuracy_rate": { $lt: threshold }
            }},
            { $sort: { "statistics.accuracy_rate": 1 } },
            { $limit: 20 },
            { $project: {
                line_id: 1,
                accuracy_rate: "$statistics.accuracy_rate",
                total_predictions: "$statistics.total_predictions",
                common_mistakes: "$statistics.common_mistakes"
            }}
        ];
        
        return await this.mongodb.collection('learning_statistics').aggregate(pipeline).toArray();
    }
    
    async generateKeywordEffectivenessReport() {
        const pipeline = [
            { $unwind: { path: "$keyword_performance.effective_keywords", preserveNullAndEmptyArrays: true } },
            { $group: {
                _id: "$keyword_performance.effective_keywords.k",
                total_usage: { $sum: 1 },
                avg_success_rate: { $avg: "$keyword_performance.effective_keywords.v" },
                lines_using: { $addToSet: "$line_id" }
            }},
            { $match: { avg_success_rate: { $gte: 0.8 } } },
            { $sort: { avg_success_rate: -1 } },
            { $limit: 50 }
        ];
        
        return await this.mongodb.collection('learning_statistics').aggregate(pipeline).toArray();
    }
    
    async predictOptimalWeights(lineId) {
        // éå»30æ—¥é–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿åˆ†æ
        const thirtyDaysAgo = new Date();
        thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30);
        
        const feedbackData = await this.mongodb.collection('user_feedback').find({
            'analysis_result.predicted_line_id': lineId,
            timestamp: { $gte: thirtyDaysAgo }
        }).toArray();
        
        if (feedbackData.length < 5) {
            return null;  // ä¸ååˆ†ãªãƒ‡ãƒ¼ã‚¿
        }
        
        // æˆåŠŸãƒ»å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        const successful = feedbackData.filter(fb => fb.user_feedback.rating >= 4);
        const unsuccessful = feedbackData.filter(fb => fb.user_feedback.rating < 4);
        
        const successfulKeywords = this.extractKeywordPatterns(successful);
        const unsuccessfulKeywords = this.extractKeywordPatterns(unsuccessful);
        
        // é‡ã¿æœ€é©åŒ–ææ¡ˆ
        const optimizedWeights = {};
        for (const keyword of Object.keys(successfulKeywords)) {
            const successRate = successfulKeywords[keyword].frequency / feedbackData.length;
            const failureRate = unsuccessfulKeywords[keyword]?.frequency || 0;
            
            if (successRate > failureRate) {
                optimizedWeights[keyword] = Math.min(2.0, successRate * 1.5);
            } else {
                optimizedWeights[keyword] = Math.max(0.1, successRate * 0.5);
            }
        }
        
        return optimizedWeights;
    }
}
```

---

## 5. ãƒªã‚¹ã‚¯ç®¡ç†ãƒ»å“è³ªä¿è¨¼

### 5.1 æŠ€è¡“çš„ãƒªã‚¹ã‚¯ã¨å¯¾ç­–

| ãƒªã‚¹ã‚¯ | ç™ºç”Ÿç¢ºç‡ | å½±éŸ¿åº¦ | å¯¾ç­– |
|--------|---------|-------|------|
| ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ€§èƒ½å•é¡Œ | ä¸­ | é«˜ | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ãƒ»ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ« |
| MeCabè¾æ›¸äº’æ›æ€§ | ä½ | ä¸­ | è¤‡æ•°è¾æ›¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œ |
| WordNetæ—¥æœ¬èªå¯¾å¿œåˆ¶é™ | ä¸­ | ä¸­ | ã‚«ã‚¹ã‚¿ãƒ é¡ç¾©èªè¾æ›¸è£œå®Œ |
| MongoDBå­¦ç¿’ãƒ‡ãƒ¼ã‚¿è‚¥å¤§åŒ– | é«˜ | ä¸­ | TTLãƒ»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ©Ÿèƒ½å®Ÿè£… |
| Redisé«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | ä¸­ | ä¸­ | é©åˆ‡ãªTTLãƒ»LRUè¨­å®š |

### 5.2 å“è³ªã‚²ãƒ¼ãƒˆ

å„ãƒ•ã‚§ãƒ¼ã‚ºã®å®Œäº†æ¡ä»¶:
- **Phase 1**: å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸãƒ»å¥åº·æ€§ãƒã‚§ãƒƒã‚¯é€šé
- **Phase 2**: MeCabè§£æç²¾åº¦90%ä»¥ä¸Šãƒ»é¡ç¾©èªãƒãƒƒãƒãƒ³ã‚°70%ä»¥ä¸Š
- **Phase 3**: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—å‹•ä½œãƒ»å­¦ç¿’åŠ¹æœç¢ºèª
- **Phase 4**: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“5msä»¥ä¸‹ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡80%ä»¥ä¸Š

---

## 6. æ‰¿èª

| å½¹å‰² | æ°å | æ‰¿èªæ—¥ | ç½²å |
|------|------|--------|------|
| ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ | | | |
| ãƒ†ãƒƒã‚¯ãƒªãƒ¼ãƒ‰ | | | |
| ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†è€… | | | |

---

**æ–‡æ›¸ç®¡ç†**
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆã«ç‰¹åŒ–ã—ãŸã‚¿ã‚¹ã‚¯åˆ†è§£
- 8é€±é–“ã§ã®æ®µéšçš„å®Ÿè£…
- é…å¸ƒå…ˆ: é–‹ç™ºãƒãƒ¼ãƒ ã€DBAãƒãƒ¼ãƒ 