# 📊 384爻システム文脈理解機能 実装計画書（データベース統合版）

**文書番号**: IP-384-003  
**バージョン**: 3.0  
**作成日**: 2025年8月28日  
**作成者**: HAQEI開発チーム  
**承認者**: [未承認]  

---

## 1. 実装概要

### 1.1 現状と目標

| 項目 | 現状 | 目標 |
|------|------|------|
| データベース設計 | JSON読み込みのみ | SQLite + MongoDB + Redis + InfluxDB |
| 形態素解析 | 基本的なキーワード抽出 | MeCab + IPADIC統合 |
| 類義語処理 | なし | WordNet日本語版統合 |
| 学習機能 | なし | ユーザーフィードバック学習 |
| 性能監視 | なし | InfluxDB + Grafana監視 |
| 分類精度 | 20%（機械的割当） | 85%以上（インテリジェント分析） |
| カバー率 | 5.7% | 60%以上 |
| 処理時間 | 2-3ms | 5ms以内（キャッシュ活用） |

### 1.2 実装スケジュール（8週間）

```
Week 1: 環境構築・基盤整備
├─ Day 1-2: データベース環境構築（SQLite, MongoDB, Redis, InfluxDB）
├─ Day 3-4: MeCab + IPADICセットアップ
└─ Day 5: 基本接続テスト・統合テスト環境準備

Week 2-3: SQLite辞書システム実装
├─ Week 2: 形態素解析辞書構築・MeCab統合
└─ Week 3: WordNet日本語版統合・類義語検索システム

Week 4-5: MongoDB学習システム実装  
├─ Week 4: フィードバック収集・統計分析システム
└─ Week 5: 動的重み調整・学習アルゴリズム実装

Week 6-7: キャッシュ・監視システム実装
├─ Week 6: Redis戦略的キャッシュシステム
└─ Week 7: InfluxDB性能監視・Grafanaダッシュボード

Week 8: 統合テスト・本番展開
├─ Day 1-3: 包括的結合テスト・性能テスト
├─ Day 4: セキュリティ監査・最終検証
└─ Day 5: 本番環境展開・切替
```

---

## 2. Week 1: 環境構築・基盤整備

### Day 1-2: データベース環境構築

#### SQLite + SQLCipher セットアップ

```bash
#!/bin/bash
# sqlite-setup.sh

echo "🔧 SQLite + SQLCipher環境構築開始..."

# SQLCipher インストール
sudo apt-get update
sudo apt-get install -y build-essential libssl-dev
wget https://github.com/sqlcipher/sqlcipher/archive/v4.5.4.tar.gz
tar -xzf v4.5.4.tar.gz
cd sqlcipher-4.5.4
./configure --enable-tempstore=yes CFLAGS="-DSQLITE_HAS_CODEC" LDFLAGS="-lcrypto"
make && sudo make install

# データベースディレクトリ作成
mkdir -p /haqei-analyzer/databases/sqlite

# 暗号化テストデータベース作成
sqlcipher /haqei-analyzer/databases/sqlite/test.db <<EOF
PRAGMA key = 'test-encryption-key';
CREATE TABLE test(id INTEGER PRIMARY KEY, data TEXT);
INSERT INTO test VALUES(1, 'encrypted data test');
SELECT * FROM test;
.exit
EOF

echo "✅ SQLite + SQLCipher セットアップ完了"
```

#### MongoDB セットアップ

```bash
#!/bin/bash
# mongodb-setup.sh

echo "🔧 MongoDB環境構築開始..."

# MongoDB Community Server インストール
curl -fsSL https://pgp.mongodb.com/server-7.0.asc | sudo gpg --dearmor -o /usr/share/keyrings/mongodb-server-7.0.gpg
echo "deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list
sudo apt-get update
sudo apt-get install -y mongodb-org

# MongoDB 設定ファイル作成
sudo tee /etc/mongod.conf > /dev/null <<EOF
storage:
  dbPath: /haqei-analyzer/databases/mongodb/data
  journal:
    enabled: true

systemLog:
  destination: file
  logAppend: true
  path: /var/log/mongodb/mongod.log

net:
  port: 27017
  bindIp: 127.0.0.1

security:
  authorization: enabled

replication:
  replSetName: "haqei-rs"
EOF

# データディレクトリ作成
sudo mkdir -p /haqei-analyzer/databases/mongodb/data
sudo chown mongodb:mongodb /haqei-analyzer/databases/mongodb/data

# サービス開始
sudo systemctl enable mongod
sudo systemctl start mongod

# 管理ユーザー作成
mongo --eval "
db = db.getSiblingDB('admin');
db.createUser({
  user: 'haqei_admin',
  pwd: 'secure_password_123',
  roles: [
    { role: 'userAdminAnyDatabase', db: 'admin' },
    { role: 'dbAdminAnyDatabase', db: 'admin' },
    { role: 'readWriteAnyDatabase', db: 'admin' }
  ]
});
"

echo "✅ MongoDB セットアップ完了"
```

#### Redis セットアップ

```bash
#!/bin/bash
# redis-setup.sh

echo "🔧 Redis環境構築開始..."

# Redis Server インストール
sudo apt-get install -y redis-server

# Redis設定ファイル作成
sudo tee /etc/redis/redis.conf > /dev/null <<EOF
# 基本設定
bind 127.0.0.1
port 6379
daemonize yes
pidfile /var/run/redis/redis-server.pid
timeout 0
tcp-keepalive 300

# メモリ設定
maxmemory 50mb
maxmemory-policy allkeys-lru

# セキュリティ
requirepass haqei-redis-secure-password

# 永続化
save 900 1
save 300 10
save 60 10000
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /haqei-analyzer/databases/redis

# AOF
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
EOF

# データディレクトリ作成
sudo mkdir -p /haqei-analyzer/databases/redis
sudo chown redis:redis /haqei-analyzer/databases/redis

# サービス再起動
sudo systemctl restart redis-server
sudo systemctl enable redis-server

# 接続テスト
redis-cli -a haqei-redis-secure-password ping

echo "✅ Redis セットアップ完了"
```

#### InfluxDB + Grafana セットアップ

```bash
#!/bin/bash
# influxdb-grafana-setup.sh

echo "🔧 InfluxDB + Grafana環境構築開始..."

# InfluxDB インストール
curl -sL https://repos.influxdata.com/influxdb.key | sudo apt-key add -
echo "deb https://repos.influxdata.com/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/influxdb.list
sudo apt-get update
sudo apt-get install -y influxdb

# InfluxDB 設定
sudo tee /etc/influxdb/influxdb.conf > /dev/null <<EOF
[meta]
  dir = "/haqei-analyzer/databases/influx/meta"

[data]
  dir = "/haqei-analyzer/databases/influx/data"
  wal-dir = "/haqei-analyzer/databases/influx/wal"

[http]
  enabled = true
  bind-address = ":8086"
  auth-enabled = true

[[collectd]]
  enabled = true
  bind-address = ":25826"
  database = "haqei_metrics"
EOF

# データディレクトリ作成
sudo mkdir -p /haqei-analyzer/databases/influx/{meta,data,wal}
sudo chown -R influxdb:influxdb /haqei-analyzer/databases/influx

# サービス開始
sudo systemctl enable influxdb
sudo systemctl start influxdb

# データベース・ユーザー作成
influx -execute "CREATE DATABASE haqei_metrics"
influx -execute "CREATE USER haqei_monitor WITH PASSWORD 'monitor123' WITH ALL PRIVILEGES"

# Grafana インストール
sudo apt-get install -y software-properties-common
sudo add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
curl https://packages.grafana.com/gpg.key | sudo apt-key add -
sudo apt-get update
sudo apt-get install -y grafana

# Grafana 自動起動設定
sudo systemctl enable grafana-server
sudo systemctl start grafana-server

echo "✅ InfluxDB + Grafana セットアップ完了"
echo "📊 Grafana: http://localhost:3000 (admin/admin)"
echo "📈 InfluxDB: http://localhost:8086"
```

### Day 3-4: MeCab + IPADIC セットアップ

#### MeCab環境構築

```bash
#!/bin/bash
# mecab-setup.sh

echo "🔧 MeCab + IPADIC環境構築開始..."

# MeCab インストール
sudo apt-get install -y mecab libmecab-dev mecab-ipadic-utf8

# IPADIC辞書追加インストール
cd /tmp
wget https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7MWVlSDBCSXZMTXM -O mecab-ipadic-2.7.0-20070801.tar.gz
tar -xzf mecab-ipadic-2.7.0-20070801.tar.gz
cd mecab-ipadic-2.7.0-20070801
./configure --with-charset=utf8
make
sudo make install

# ディレクトリ作成
mkdir -p /haqei-analyzer/dictionaries/ipadic
mkdir -p /haqei-analyzer/dictionaries/custom

# IPADICデータをSQLiteに変換するスクリプト
cat > /haqei-analyzer/scripts/ipadic-to-sqlite.py << 'EOF'
#!/usr/bin/env python3
import sqlite3
import csv
import os

def convert_ipadic_to_sqlite():
    # SQLite接続
    conn = sqlite3.connect('/haqei-analyzer/databases/sqlite/morphology.db')
    cursor = conn.cursor()
    
    # テーブル作成
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS morphology_dict (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            word TEXT NOT NULL,
            reading TEXT,
            part_of_speech TEXT NOT NULL,
            pos_detail1 TEXT,
            pos_detail2 TEXT,
            pos_detail3 TEXT,
            inflection_type TEXT,
            inflection_form TEXT,
            base_form TEXT,
            pronunciation TEXT,
            frequency INTEGER DEFAULT 1,
            semantic_category TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # IPADICファイル読み込み
    ipadic_path = '/usr/local/lib/mecab/dic/ipadic'
    csv_files = ['Noun.csv', 'Verb.csv', 'Adj.csv', 'Adverb.csv', 'Others.csv']
    
    total_entries = 0
    for csv_file in csv_files:
        file_path = os.path.join(ipadic_path, csv_file)
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='euc-jp') as f:
                reader = csv.reader(f)
                for row in reader:
                    if len(row) >= 11:
                        cursor.execute('''
                            INSERT INTO morphology_dict 
                            (word, reading, part_of_speech, pos_detail1, pos_detail2, 
                             pos_detail3, inflection_type, inflection_form, base_form, pronunciation)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (row[0], row[7], row[4], row[5], row[6], row[7], row[8], row[9], row[10], row[11]))
                        total_entries += 1
    
    # インデックス作成
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_word_lookup ON morphology_dict(word)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_pos_category ON morphology_dict(part_of_speech, pos_detail1)')
    
    conn.commit()
    conn.close()
    print(f'✅ {total_entries}件のIPADICデータをSQLiteに変換完了')

if __name__ == '__main__':
    convert_ipadic_to_sqlite()
EOF

# スクリプト実行権限付与・実行
chmod +x /haqei-analyzer/scripts/ipadic-to-sqlite.py
python3 /haqei-analyzer/scripts/ipadic-to-sqlite.py

echo "✅ MeCab + IPADIC セットアップ完了"
```

#### Node.js MeCabライブラリテスト

```javascript
// mecab-test.js
const MeCab = require('mecab-async');
const sqlite3 = require('sqlite3').verbose();

class MeCabTester {
    constructor() {
        this.mecab = new MeCab();
        this.db = new sqlite3.Database('/haqei-analyzer/databases/sqlite/morphology.db');
    }
    
    async testBasicAnalysis() {
        console.log('🧪 MeCab基本解析テスト開始...');
        
        const testTexts = [
            'リーダーシップを発揮して責任を持つ',
            '新しいプロジェクトを始める時期',
            '困難な問題に直面している状況'
        ];
        
        for (const text of testTexts) {
            console.log(`\n📝 入力: "${text}"`);
            
            const result = await this.mecab.parse(text);
            console.log('🔍 形態素解析結果:');
            
            result.forEach((token, index) => {
                if (token[0] !== 'EOS') {
                    console.log(`  ${index + 1}: ${token[0]} (${token[1]}) - ${token[2]}`);
                }
            });
            
            // データベースから語彙情報取得
            const keywords = await this.extractKeywordsFromDB(result);
            console.log('📚 データベース拡張情報:', keywords);
        }
    }
    
    async extractKeywordsFromDB(tokens) {
        const keywords = [];
        for (const token of tokens) {
            if (token[0] !== 'EOS' && ['名詞', '動詞', '形容詞'].includes(token[1])) {
                const dbInfo = await this.queryDB(token[0]);
                if (dbInfo) {
                    keywords.push({
                        word: token[0],
                        pos: token[1],
                        frequency: dbInfo.frequency,
                        semantic_category: dbInfo.semantic_category
                    });
                }
            }
        }
        return keywords;
    }
    
    queryDB(word) {
        return new Promise((resolve, reject) => {
            this.db.get(
                'SELECT * FROM morphology_dict WHERE word = ? LIMIT 1',
                [word],
                (err, row) => {
                    if (err) reject(err);
                    else resolve(row);
                }
            );
        });
    }
}

// テスト実行
(async () => {
    const tester = new MeCabTester();
    await tester.testBasicAnalysis();
    console.log('\n✅ MeCabテスト完了');
})();
```

### Day 5: 基本接続テスト・統合テスト環境準備

#### 統合接続テストシステム

```javascript
// integration-test.js
class DatabaseIntegrationTester {
    constructor() {
        this.results = {};
    }
    
    async runAllTests() {
        console.log('🧪 データベース統合テスト開始...\n');
        
        // 各データベース接続テスト
        await this.testSQLiteConnection();
        await this.testMongoDBConnection();
        await this.testRedisConnection();
        await this.testInfluxDBConnection();
        
        // 統合テスト
        await this.testDataFlow();
        await this.testPerformance();
        
        // 結果レポート
        this.generateReport();
    }
    
    async testSQLiteConnection() {
        console.log('📄 SQLite接続テスト...');
        try {
            const sqlite3 = require('sqlite3').verbose();
            const db = new sqlite3.Database('/haqei-analyzer/databases/sqlite/morphology.db');
            
            const result = await new Promise((resolve, reject) => {
                db.get('SELECT COUNT(*) as count FROM morphology_dict', (err, row) => {
                    err ? reject(err) : resolve(row);
                });
            });
            
            this.results.sqlite = {
                status: 'success',
                records: result.count,
                message: `${result.count}件のレコードを確認`
            };
            console.log(`  ✅ SUCCESS: ${result.count}件のレコードを確認`);
            
        } catch (error) {
            this.results.sqlite = {
                status: 'error',
                message: error.message
            };
            console.log(`  ❌ ERROR: ${error.message}`);
        }
    }
    
    async testMongoDBConnection() {
        console.log('🍃 MongoDB接続テスト...');
        try {
            const { MongoClient } = require('mongodb');
            const client = new MongoClient('mongodb://haqei_admin:secure_password_123@localhost:27017', {
                authSource: 'admin'
            });
            
            await client.connect();
            const db = client.db('haqei_analyzer');
            
            // テストコレクション作成
            const testCollection = db.collection('connection_test');
            const insertResult = await testCollection.insertOne({
                timestamp: new Date(),
                test_data: 'integration_test'
            });
            
            const findResult = await testCollection.findOne({ _id: insertResult.insertedId });
            
            await client.close();
            
            this.results.mongodb = {
                status: 'success',
                message: 'データ挿入・検索成功'
            };
            console.log('  ✅ SUCCESS: データ挿入・検索成功');
            
        } catch (error) {
            this.results.mongodb = {
                status: 'error',
                message: error.message
            };
            console.log(`  ❌ ERROR: ${error.message}`);
        }
    }
    
    async testRedisConnection() {
        console.log('📦 Redis接続テスト...');
        try {
            const redis = require('redis');
            const client = redis.createClient({
                host: 'localhost',
                port: 6379,
                password: 'haqei-redis-secure-password'
            });
            
            await client.connect();
            
            // テストデータ設定・取得
            await client.set('test_key', 'test_value', { EX: 60 });
            const result = await client.get('test_key');
            
            if (result === 'test_value') {
                this.results.redis = {
                    status: 'success',
                    message: 'キャッシュ動作確認'
                };
                console.log('  ✅ SUCCESS: キャッシュ動作確認');
            } else {
                throw new Error('データ不整合');
            }
            
            await client.disconnect();
            
        } catch (error) {
            this.results.redis = {
                status: 'error',
                message: error.message
            };
            console.log(`  ❌ ERROR: ${error.message}`);
        }
    }
    
    async testInfluxDBConnection() {
        console.log('📈 InfluxDB接続テスト...');
        try {
            const Influx = require('influx');
            const influx = new Influx.InfluxDB({
                host: 'localhost',
                database: 'haqei_metrics',
                username: 'haqei_monitor',
                password: 'monitor123'
            });
            
            // テストメトリクス書き込み
            await influx.writePoints([{
                measurement: 'system_test',
                tags: { test_type: 'integration' },
                fields: { value: 1, response_time: 0.5 },
                timestamp: new Date()
            }]);
            
            // データ読み込み確認
            const results = await influx.query('SELECT * FROM system_test WHERE test_type = \'integration\' LIMIT 1');
            
            if (results.length > 0) {
                this.results.influxdb = {
                    status: 'success',
                    message: 'メトリクス書き込み・読み込み確認'
                };
                console.log('  ✅ SUCCESS: メトリクス書き込み・読み込み確認');
            } else {
                throw new Error('データなし');
            }
            
        } catch (error) {
            this.results.influxdb = {
                status: 'error',
                message: error.message
            };
            console.log(`  ❌ ERROR: ${error.message}`);
        }
    }
    
    generateReport() {
        console.log('\n📊 統合テスト結果レポート:');
        console.log('==========================================');
        
        let successCount = 0;
        let totalCount = 0;
        
        for (const [system, result] of Object.entries(this.results)) {
            totalCount++;
            const status = result.status === 'success' ? '✅ 成功' : '❌ 失敗';
            console.log(`${system.toUpperCase()}: ${status} - ${result.message}`);
            if (result.status === 'success') successCount++;
        }
        
        console.log('==========================================');
        console.log(`成功率: ${successCount}/${totalCount} (${Math.round(successCount/totalCount*100)}%)`);
        
        if (successCount === totalCount) {
            console.log('🎉 全システム統合テスト成功！Week 2開発開始可能');
        } else {
            console.log('⚠️ 一部システムに問題があります。修正してから進行してください');
        }
    }
}

// テスト実行
(async () => {
    const tester = new DatabaseIntegrationTester();
    await tester.runAllTests();
})();
```

---

## 3. Week 2-3: SQLite辞書システム実装

### Week 2: 形態素解析辞書構築・MeCab統合

#### 高度なMeCab統合システム

```javascript
// AdvancedMeCabAnalyzer.js
class AdvancedMeCabAnalyzer {
    constructor(databaseManager) {
        this.mecab = require('mecab-async');
        this.db = databaseManager.connections.sqlite;
        this.cache = new Map();
    }
    
    async analyzeWithContext(text, domain = 'general') {
        // キャッシュ確認
        const cacheKey = `${text}:${domain}`;
        if (this.cache.has(cacheKey)) {
            return this.cache.get(cacheKey);
        }
        
        console.log(`🔍 高度形態素解析開始: "${text}" (ドメイン: ${domain})`);
        
        // 1. 基本形態素解析
        const tokens = await this.mecab.parse(text);
        console.log(`  📝 基本トークン数: ${tokens.length}`);
        
        // 2. データベース拡張解析
        const enrichedTokens = await this.enrichWithDatabase(tokens, domain);
        console.log(`  📚 データベース拡張完了`);
        
        // 3. 専門用語識別
        const specializedTerms = await this.identifySpecializedTerms(enrichedTokens, domain);
        console.log(`  🎯 専門用語識別: ${specializedTerms.length}個`);
        
        // 4. セマンティック解析
        const semanticAnalysis = await this.performSemanticAnalysis(enrichedTokens);
        console.log(`  🧠 セマンティック解析完了`);
        
        const result = {
            original_text: text,
            domain: domain,
            tokens: enrichedTokens,
            specialized_terms: specializedTerms,
            semantic_analysis: semanticAnalysis,
            keywords: this.extractSmartKeywords(enrichedTokens, specializedTerms),
            processing_stats: {
                total_tokens: tokens.length,
                enriched_tokens: enrichedTokens.length,
                specialized_terms: specializedTerms.length,
                processing_time: Date.now()
            }
        };
        
        // キャッシュ保存（1時間）
        setTimeout(() => this.cache.delete(cacheKey), 3600000);
        this.cache.set(cacheKey, result);
        
        return result;
    }
    
    async enrichWithDatabase(tokens, domain) {
        const enriched = [];
        
        for (const token of tokens) {
            if (token[0] === 'EOS') continue;
            
            const baseToken = {
                surface: token[0],
                pos: token[1],
                pos_detail1: token[2],
                pos_detail2: token[3],
                pos_detail3: token[4],
                inflection_type: token[5],
                inflection_form: token[6],
                base_form: token[7],
                reading: token[8],
                pronunciation: token[9]
            };
            
            // データベースから詳細情報取得
            const dbInfo = await this.queryMorphologyDB(baseToken.base_form || baseToken.surface);
            
            // ドメイン固有の専門用語チェック
            const domainInfo = await this.queryDomainSpecificDB(baseToken.base_form || baseToken.surface, domain);
            
            enriched.push({
                ...baseToken,
                frequency: dbInfo?.frequency || 1,
                semantic_category: dbInfo?.semantic_category,
                domain_weight: domainInfo?.weight || 1.0,
                is_specialized: !!domainInfo,
                domain_synonyms: domainInfo?.synonyms || []
            });
        }
        
        return enriched;
    }
    
    async queryMorphologyDB(word) {
        return new Promise((resolve, reject) => {
            this.db.get(`
                SELECT frequency, semantic_category, pos_detail1, pos_detail2
                FROM morphology_dict 
                WHERE word = ? OR base_form = ?
                ORDER BY frequency DESC 
                LIMIT 1
            `, [word, word], (err, row) => {
                if (err) reject(err);
                else resolve(row);
            });
        });
    }
    
    async queryDomainSpecificDB(word, domain) {
        return new Promise((resolve, reject) => {
            this.db.get(`
                SELECT weight, synonyms, domain
                FROM specialized_terms 
                WHERE term = ? AND domain IN (?, 'general')
                ORDER BY weight DESC
                LIMIT 1
            `, [word, domain], (err, row) => {
                if (err) reject(err);
                else resolve(row ? {
                    ...row,
                    synonyms: JSON.parse(row.synonyms || '[]')
                } : null);
            });
        });
    }
    
    async identifySpecializedTerms(tokens, domain) {
        const specialized = [];
        
        for (const token of tokens) {
            if (token.is_specialized || 
                (token.domain_weight > 1.5) ||
                (['乾', '坤', '震', '巽', '坎', '離', '艮', '兌'].includes(token.surface))) {
                
                specialized.push({
                    term: token.surface,
                    base_form: token.base_form,
                    domain: domain,
                    weight: token.domain_weight,
                    category: token.semantic_category || 'unknown',
                    synonyms: token.domain_synonyms
                });
            }
        }
        
        return specialized;
    }
    
    extractSmartKeywords(tokens, specializedTerms) {
        const keywords = [];
        
        // 重要品詞のフィルタリング
        const importantTokens = tokens.filter(token => 
            ['名詞', '動詞', '形容詞'].includes(token.pos) &&
            token.surface.length > 1 &&
            !['する', 'ある', 'いる', 'なる'].includes(token.base_form)
        );
        
        for (const token of importantTokens) {
            let weight = this.calculateTokenWeight(token);
            
            // 専門用語ボーナス
            const specialized = specializedTerms.find(st => st.term === token.surface);
            if (specialized) {
                weight *= specialized.weight;
            }
            
            keywords.push({
                word: token.base_form || token.surface,
                pos: token.pos,
                weight: weight,
                semantic_category: token.semantic_category,
                is_specialized: !!specialized,
                reading: token.reading
            });
        }
        
        // 重みでソート
        return keywords.sort((a, b) => b.weight - a.weight);
    }
    
    calculateTokenWeight(token) {
        let weight = 1.0;
        
        // 品詞重み
        const posWeights = {
            '名詞': 1.2,
            '動詞': 1.1,
            '形容詞': 1.0,
            '副詞': 0.8
        };
        weight *= (posWeights[token.pos] || 0.5);
        
        // 頻度による重み（逆相関 - 珍しい語ほど重要）
        if (token.frequency > 1) {
            weight *= Math.log(100000 / token.frequency);
        }
        
        // ドメイン特化重み
        weight *= token.domain_weight;
        
        // 長さによる重み調整
        if (token.surface.length >= 3) weight *= 1.1;
        if (token.surface.length >= 5) weight *= 1.2;
        
        return Math.max(0.1, Math.min(3.0, weight));
    }
}
```

### Week 3: WordNet日本語版統合・類義語検索システム

#### WordNet統合システム

```python
# wordnet-integration.py
import sqlite3
import json
import requests
from collections import defaultdict

class WordNetIntegrator:
    def __init__(self, db_path):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.setup_database()
    
    def setup_database(self):
        """類義語関係テーブルセットアップ"""
        cursor = self.conn.cursor()
        
        # 類義語関係テーブル
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS synonym_relations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                word1 TEXT NOT NULL,
                word2 TEXT NOT NULL,
                similarity_score REAL NOT NULL CHECK(similarity_score >= 0.0 AND similarity_score <= 1.0),
                relation_type TEXT NOT NULL,
                source TEXT DEFAULT 'wordnet',
                confidence REAL DEFAULT 1.0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # WordNet概念テーブル
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS wordnet_concepts (
                synset_id TEXT PRIMARY KEY,
                words TEXT NOT NULL,
                pos TEXT NOT NULL,
                definition TEXT,
                examples TEXT,
                hypernyms TEXT,
                hyponyms TEXT,
                meronyms TEXT,
                holonyms TEXT
            )
        ''')
        
        # インデックス作成
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_word1_score ON synonym_relations(word1, similarity_score DESC)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_word2_score ON synonym_relations(word2, similarity_score DESC)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_synset_pos ON wordnet_concepts(pos)')
        
        self.conn.commit()
        print("✅ WordNet データベースセットアップ完了")
    
    def download_japanese_wordnet(self):
        """日本語WordNetデータをダウンロード・解析"""
        print("🔄 日本語WordNetデータダウンロード開始...")
        
        # 日本語WordNetプロジェクトからデータ取得
        wordnet_url = "http://compling.hss.ntu.edu.sg/wnja/data/1.1/wnjpn.db.gz"
        
        try:
            import gzip
            import urllib.request
            
            # ダウンロード
            urllib.request.urlretrieve(wordnet_url, "/tmp/wnjpn.db.gz")
            
            # 解凍
            with gzip.open('/tmp/wnjpn.db.gz', 'rb') as f_in:
                with open('/tmp/wnjpn.db', 'wb') as f_out:
                    f_out.write(f_in.read())
            
            print("✅ 日本語WordNetダウンロード完了")
            return True
            
        except Exception as e:
            print(f"❌ ダウンロード失敗: {e}")
            return False
    
    def process_wordnet_data(self):
        """WordNetデータを処理してSQLiteに統合"""
        print("🔄 WordNetデータ処理開始...")
        
        # 日本語WordNet SQLiteファイル読み込み
        wordnet_db = sqlite3.connect('/tmp/wnjpn.db')
        cursor = wordnet_db.cursor()
        
        # シンセット情報取得
        cursor.execute('''
            SELECT synset, lemma, pos, def_jp, def_en
            FROM synset 
            LEFT JOIN synset_def ON synset.synset = synset_def.synset
            WHERE lang = 'jpn'
        ''')
        
        synsets = defaultdict(list)
        synset_info = {}
        
        for row in cursor.fetchall():
            synset_id, lemma, pos, def_jp, def_en = row
            synsets[synset_id].append(lemma)
            synset_info[synset_id] = {
                'pos': pos,
                'definition_jp': def_jp,
                'definition_en': def_en
            }
        
        # 類義語関係をデータベースに保存
        self.save_synonym_relations(synsets, synset_info)
        
        wordnet_db.close()
        print("✅ WordNetデータ処理完了")
    
    def save_synonym_relations(self, synsets, synset_info):
        """類義語関係をデータベースに保存"""
        cursor = self.conn.cursor()
        saved_count = 0
        
        for synset_id, words in synsets.items():
            if len(words) < 2:
                continue
                
            # 同一シンセット内の語彙は類義語として保存
            for i, word1 in enumerate(words):
                for j, word2 in enumerate(words):
                    if i != j:
                        cursor.execute('''
                            INSERT OR IGNORE INTO synonym_relations
                            (word1, word2, similarity_score, relation_type, source, confidence)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (word1, word2, 1.0, 'synonym', 'wordnet_jp', 1.0))
                        saved_count += 1
            
            # シンセット情報保存
            info = synset_info.get(synset_id, {})
            cursor.execute('''
                INSERT OR REPLACE INTO wordnet_concepts
                (synset_id, words, pos, definition, examples)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                synset_id,
                json.dumps(words, ensure_ascii=False),
                info.get('pos', ''),
                info.get('definition_jp', ''),
                json.dumps([], ensure_ascii=False)  # 例文は後で追加
            ))
        
        self.conn.commit()
        print(f"✅ {saved_count}件の類義語関係を保存")
    
    def add_custom_yijing_synonyms(self):
        """易経専門用語の類義語関係を追加"""
        print("🔄 易経専門用語類義語追加...")
        
        # 易経特化類義語辞書
        yijing_synonyms = {
            '乾': ['天', '創造', '活力', 'リーダーシップ', '父'],
            '坤': ['地', '受容', '育成', '母性', '協力'],
            '震': ['雷', '動き', '行動', '長男', '活発'],
            '巽': ['風', '浸透', '影響', '長女', '柔軟'],
            '坎': ['水', '困難', '試練', '中男', '危険'],
            '離': ['火', '明智', '理解', '中女', '明るさ'],
            '艮': ['山', '静止', '安定', '少男', '堅実'],
            '兌': ['沢', '喜び', '交流', '少女', '楽しさ'],
            'リーダー': ['指導者', '統率者', '責任者', '指揮者', '先導者'],
            '成長': ['発達', '進歩', '発展', '向上', '成熟'],
            '困難': ['試練', '障害', '問題', '苦労', '挑戦'],
            '始まり': ['開始', 'スタート', '出発', '初期', '第一歩']
        }
        
        cursor = self.conn.cursor()
        saved_count = 0
        
        for main_word, synonyms in yijing_synonyms.items():
            for synonym in synonyms:
                # 双方向の類義語関係を作成
                for word1, word2, score in [(main_word, synonym, 0.9), (synonym, main_word, 0.9)]:
                    cursor.execute('''
                        INSERT OR IGNORE INTO synonym_relations
                        (word1, word2, similarity_score, relation_type, source, confidence)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (word1, word2, score, 'custom_synonym', 'yijing_domain', 1.0))
                    saved_count += 1
                
                # 同義語間の関係も作成（低い重み）
                for i, syn1 in enumerate(synonyms):
                    for j, syn2 in enumerate(synonyms):
                        if i != j:
                            cursor.execute('''
                                INSERT OR IGNORE INTO synonym_relations
                                (word1, word2, similarity_score, relation_type, source, confidence)
                                VALUES (?, ?, ?, ?, ?, ?)
                            ''', (syn1, syn2, 0.7, 'related_synonym', 'yijing_domain', 0.8))
                            saved_count += 1
        
        self.conn.commit()
        print(f"✅ {saved_count}件の易経専門類義語を追加")

# 実行スクリプト
def main():
    integrator = WordNetIntegrator('/haqei-analyzer/databases/sqlite/synonyms.db')
    
    # WordNetデータ統合
    if integrator.download_japanese_wordnet():
        integrator.process_wordnet_data()
    
    # 易経専門用語追加
    integrator.add_custom_yijing_synonyms()
    
    print("🎉 WordNet統合システム完了！")

if __name__ == '__main__':
    main()
```

---

## 4. 成功指標と品質ゲート

### Week 1完了条件
- [ ] SQLite + SQLCipher: 暗号化データベース動作確認
- [ ] MongoDB: 認証・レプリカセット動作確認
- [ ] Redis: セキュア接続・キャッシュ動作確認  
- [ ] InfluxDB: メトリクス収集・Grafana連携確認
- [ ] MeCab + IPADIC: 50,000語以上の辞書データ変換
- [ ] 統合テスト: 全システム接続成功率100%

### Week 2-3完了条件
- [ ] 形態素解析精度: 90%以上（標準テストセット）
- [ ] 類義語マッチング: WordNet + カスタム辞書で70%以上カバー
- [ ] 専門用語識別: 易経用語95%以上認識
- [ ] 性能: 1000文字テキスト解析3ms以内
- [ ] データベース: SQLiteクエリ平均1ms以内

---

## 5. 承認

| 役割 | 氏名 | 承認日 | 署名 |
|------|------|--------|------|
| プロジェクトマネージャー | | | |
| データベース管理者 | | | |
| 自然言語処理エンジニア | | | |

---

**文書管理**
- データベース統合に特化した実装計画
- 週単位の具体的な作業スケジュール
- 配布先: 開発チーム、DBAチーム、QAチーム